{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Metal GPU acceleration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "/opt/anaconda3/envs/312/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "# Define the audio file path\n",
    "audio_file = \"audio/5760-Nano-L2.mp3\"\n",
    "\n",
    "# Check if Metal Performance Shaders (MPS) is available for GPU acceleration on macOS\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use MPS device if available\n",
    "    print(\"Using Metal GPU acceleration\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU if MPS is not available\n",
    "    print(\"Metal not available, using CPU\")\n",
    "\n",
    "# Set up the automatic speech recognition pipeline\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-large-v3\",  # Specify the Whisper model to use\n",
    "    torch_dtype=torch.float16,  # Use float16 for faster computation and reduced memory usage\n",
    "    device=device,  # Use the selected device (MPS or CPU)\n",
    "    return_timestamps=True,  # Return word-level timestamps in the output\n",
    ")\n",
    "\n",
    "# Start measuring the transcription time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the transcription using the pipeline\n",
    "result = pipe(audio_file)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print the transcription results\n",
    "print(f\"Transcription completed in {elapsed_time:.2f} seconds\")\n",
    "print(\"Transcription result:\")\n",
    "print(result[\"text\"])\n",
    "\n",
    "# Print the device and model data type information\n",
    "print(f\"Device: {pipe.device}\")\n",
    "print(f\"Model dtype: {pipe.model.dtype}\")\n",
    "\n",
    "\n",
    "# Define a function to process audio in chunks\n",
    "def process_in_chunks(audio_file, chunk_length_s=30):\n",
    "    from pydub import AudioSegment\n",
    "    import math\n",
    "    import os\n",
    "\n",
    "    # Load the audio file using pydub\n",
    "    audio = AudioSegment.from_mp3(audio_file)\n",
    "\n",
    "    # Calculate the number of chunks\n",
    "    chunk_length_ms = chunk_length_s * 1000  # Convert chunk length to milliseconds\n",
    "    chunks = math.ceil(\n",
    "        len(audio) / chunk_length_ms\n",
    "    )  # Calculate the number of chunks needed\n",
    "\n",
    "    transcriptions = []  # Initialize a list to store transcriptions for each chunk\n",
    "\n",
    "    # Iterate through the chunks\n",
    "    for i in range(chunks):\n",
    "        # Determine the start and end times for the current chunk\n",
    "        start_time = i * chunk_length_ms\n",
    "        end_time = (i + 1) * chunk_length_ms\n",
    "        chunk = audio[\n",
    "            start_time:end_time\n",
    "        ]  # Extract the audio chunk from the full audio\n",
    "\n",
    "        # Create a temporary file for the audio chunk\n",
    "        chunk_file = f\"temp_chunk_{i}.wav\"\n",
    "        chunk.export(\n",
    "            chunk_file, format=\"wav\"\n",
    "        )  # Export the chunk as a WAV file\n",
    "\n",
    "        # Perform transcription on the audio chunk\n",
    "        result = pipe(\n",
    "            chunk_file, language=\"en\"\n",
    "        )  # Transcribe the chunk with language specified\n",
    "\n",
    "        transcriptions.append(\n",
    "            result[\"text\"]\n",
    "        )  # Add the chunk's transcription to the list\n",
    "\n",
    "        # Clean up the temporary file\n",
    "        os.remove(chunk_file)  # Remove the temporary chunk file\n",
    "\n",
    "    # Join all the transcriptions into a single string\n",
    "    return \" \".join(\n",
    "        transcriptions\n",
    "    )  # Concatenate the transcriptions with a space in between\n",
    "\n",
    "# After processing the full transcription\n",
    "full_transcription = process_in_chunks(audio_file)\n",
    "\n",
    "# Write the full transcription to a text file\n",
    "with open('transcription.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(full_transcription)\n",
    "\n",
    "print(\"Transcription saved to 'transcription.txt'\")\n",
    "\n",
    "\n",
    "# # Process the entire audio file in chunks to handle long audio\n",
    "# full_transcription = process_in_chunks(audio_file)\n",
    "# print(\"Full transcription:\")\n",
    "# print(full_transcription)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
